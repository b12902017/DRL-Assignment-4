# -*- coding: utf-8 -*-
"""DRL-Assignment4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mq4H5VEE2Lc-ih_DDcetKXvvAbvJwCMS
"""

!pip install dm_control
!pip install shimmy

import numpy as np
import gymnasium as gym
from tqdm import tqdm
import matplotlib.pyplot as plt
from dm_control import suite
from gymnasium import spaces
from gymnasium.wrappers import FlattenObservation
from shimmy import DmControlCompatibilityV0 as DmControltoGymnasium
from dmc import make_dmc_env


def make_env():
	# Create environment with state observations
	env_name = "cartpole-balance"
	env = make_dmc_env(env_name, np.random.randint(0, 1000000), flatten=True, use_pixels=False)
	return env

import torch
import torch.nn as nn
from torch.distributions import Normal
from torchrl.data import ReplayBuffer, TensorDictReplayBuffer, LazyTensorStorage
from torchrl.data import SamplerWithoutReplacement
from torch.optim import Adam
import torch.nn.functional as F
import os

def _init_weight(m):
  if isinstance(m, nn.Linear):
    torch.nn.init.xavier_uniform_(m.weight, gain=1)
    torch.nn.init.constant_(m.bias, 0)

class CriticNet(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim):
        super(CriticNet, self).__init__()

        input_dim = state_dim + action_dim

        # Q1 architecture
        self.q1 = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )

        # Q2 architecture
        self.q2 = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )

        self.apply(_init_weight)

    def forward(self, state, action):
        xu = torch.cat([state, action], dim=1)
        x1 = self.q1(xu)
        x2 = self.q2(xu)
        return x1, x2

LOG_STD_MIN = -20
LOG_STD_MAX = 2
class ActorNet(nn.Module):
  def __init__(self, state_dim, action_dim, hidden_dim, device="cpu"):
    super(ActorNet, self).__init__()
    self.feature = nn.Sequential(
      nn.Linear(state_dim, hidden_dim),
      nn.ReLU(),
      nn.Linear(hidden_dim, hidden_dim),
      nn.ReLU()
    )

    self.mean_linear = nn.Linear(hidden_dim, action_dim)
    self.log_std_linear = nn.Linear(hidden_dim, action_dim)
    self.action_scale = torch.tensor(1.0, dtype=torch.float32, device=device)

    self.apply(_init_weight)
    self.to(device)

  def forward(self, state):
    x = self.feature(state)
    mean = self.mean_linear(x)
    log_std = self.log_std_linear(x)
    log_std = torch.clamp(log_std, min=LOG_STD_MIN, max=LOG_STD_MAX)
    return mean, log_std

  def sample(self, state):
    mean, log_std = self.forward(state)
    std = torch.exp(log_std)
    normal = Normal(mean, std)
    x_t = normal.rsample()
    y_t = torch.tanh(x_t)
    action = y_t * self.action_scale

    log_prob = normal.log_prob(x_t)
    log_prob -= torch.log(self.action_scale * (1 - y_t.pow(2)) + 1e-6)
    log_prob = log_prob.sum(dim=1, keepdim=True)

    mean = torch.tanh(mean) * self.action_scale
    return action, log_prob, mean

class MemoryBuffer:
  def __init__(self, capacity, device="cpu"):
      self.device = torch.device(device)
      self.buffer = ReplayBuffer(
          storage=LazyTensorStorage(max_size=capacity, device=self.device),
          sampler=SamplerWithoutReplacement()
      )

  def push(self, state, action, reward, next_state, done):
      transition = {
          "state": torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0),
          "action": torch.tensor(action, dtype=torch.float32, device=self.device).unsqueeze(0),
          "reward": torch.tensor(reward, dtype=torch.float32, device=self.device).unsqueeze(0),
          "next_state": torch.tensor(next_state, dtype=torch.float32, device=self.device).unsqueeze(0),
          "done": torch.tensor(done, device=self.device).unsqueeze(0)
      }
      self.buffer.add(transition)

  def sample(self, batch_size):
      batch = self.buffer.sample(batch_size)
      return (
          batch["state"],
          batch["action"],
          batch["reward"],
          batch["next_state"],
          batch["done"]
      )

  def __len__(self):
      return len(self.buffer)

  def save_buffer(self, save_path):
      torch.save(self.buffer._storage, save_path)

  def load_buffer(self, save_path):
      self.buffer._storage = torch.load(save_path)

def soft_update(target, source, tau):
    for target_param, param in zip(target.parameters(), source.parameters()):
        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)

def hard_update(target, source):
    for target_param, param in zip(target.parameters(), source.parameters()):
        target_param.data.copy_(param.data)

class SAC:
  def __init__(
        self,
        state_dim,
        action_dim,
        hidden_dim=256,
        gamma=0.99,
        tau=0.005,
        alpha=0.2,
        lr=3e-4,
        device="cpu",
    ):
    self.gamma = gamma
    self.tau = tau
    self.alpha = alpha
    self.device = torch.device(device)

    self.critic = CriticNet(state_dim, action_dim, hidden_dim).to(self.device)
    self.critic_optim = Adam(self.critic.parameters(), lr=lr)

    self.critic_target = CriticNet(state_dim, action_dim, hidden_dim).to(self.device)
    hard_update(self.critic_target, self.critic)

    self.target_entropy = -float(action_dim)
    self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)
    self.alpha_optim = Adam([self.log_alpha], lr=lr)

    self.policy = ActorNet(state_dim, action_dim, hidden_dim, device=self.device)
    self.policy_optim = Adam(self.policy.parameters(), lr=lr)

  def get_action(self, state, eval=False):
    state = torch.FloatTensor(state).to(self.device).unsqueeze(0)
    if eval is False:
        action, _, _ = self.policy.sample(state)
    else:
        _, _, action = self.policy.sample(state)
    return action.detach().cpu().numpy()[0]

  def train(self, memory, batch_size):
      state_batch, action_batch, reward_batch, next_state_batch, done_batch = memory.sample(batch_size=batch_size)

      '''
      print(f"state_batch.shape: {state_batch.shape}")
      print(f"action_batch.shape: {action_batch.shape}")
      print(f"reward_batch.shape: {reward_batch.shape}")
      print(f"next_state_batch.shape: {next_state_batch.shape}")
      print(f"done_batch.shape: {done_batch.shape}")
      '''

      state_batch = state_batch.squeeze(1)
      action_batch = action_batch.squeeze(1)
      next_state_batch = next_state_batch.squeeze(1)
      done_batch = done_batch.float()

      with torch.no_grad():
          next_state_action, next_state_log_prob, _ = self.policy.sample(next_state_batch)
          qf1_next_target, qf2_next_target = self.critic_target(next_state_batch, next_state_action)
          min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - self.alpha * next_state_log_prob
          next_q_value = reward_batch + (1-done_batch) * self.gamma * (min_qf_next_target)

      qf1, qf2 = self.critic(state_batch, action_batch)
      qf1_loss = F.mse_loss(qf1, next_q_value)
      qf2_loss = F.mse_loss(qf2, next_q_value)
      qf_loss = qf1_loss + qf2_loss

      self.critic_optim.zero_grad()
      qf_loss.backward()
      self.critic_optim.step()

      action, log_prob, _ = self.policy.sample(state_batch)

      qf1_pi, qf2_pi = self.critic(state_batch, action)
      min_qf_pi = torch.min(qf1_pi, qf2_pi)

      policy_loss = ((self.alpha * log_prob) - min_qf_pi).mean()

      self.policy_optim.zero_grad()
      policy_loss.backward()
      self.policy_optim.step()

      alpha_loss = -(self.log_alpha * (log_prob + self.target_entropy).detach()).mean()

      self.alpha_optim.zero_grad()
      alpha_loss.backward()
      self.alpha_optim.step()

      self.alpha = self.log_alpha.exp()
      alpha_tlogs = self.alpha.clone()

      soft_update(self.critic_target, self.critic, self.tau)
      return qf1_loss.item(), qf2_loss.item(), policy_loss.item(), alpha_loss.item(), alpha_tlogs.item()

  def save_checkpoint(self, env_name=None, suffix="", ckpt_path=None):
      if not os.path.exists('checkpoints/'):
          os.makedirs('checkpoints/')
      if ckpt_path is None:
          ckpt_path = "checkpoints/sac_checkpoint_{}_{}".format(env_name, suffix)
      print('Saving models to {}'.format(ckpt_path))
      torch.save({'policy_state_dict': self.policy.state_dict(),
                  'critic_state_dict': self.critic.state_dict(),
                  'critic_target_state_dict': self.critic_target.state_dict(),
                  'critic_optimizer_state_dict': self.critic_optim.state_dict(),
                  'policy_optimizer_state_dict': self.policy_optim.state_dict()}, ckpt_path)

  # Load model parameters
  def load_checkpoint(self, ckpt_path, evaluate=False):
      print('Loading models from {}'.format(ckpt_path))
      if ckpt_path is not None:
          checkpoint = torch.load(ckpt_path)
          self.policy.load_state_dict(checkpoint['policy_state_dict'])
          self.critic.load_state_dict(checkpoint['critic_state_dict'])
          self.critic_target.load_state_dict(checkpoint['critic_target_state_dict'])
          self.critic_optim.load_state_dict(checkpoint['critic_optimizer_state_dict'])
          self.policy_optim.load_state_dict(checkpoint['policy_optimizer_state_dict'])

          if evaluate:
              self.policy.eval()
              self.critic.eval()
              self.critic_target.eval()
          else:
              self.policy.train()
              self.critic.train()
              self.critic_target.train()

env = make_env()
state_dim = env.observation_space.shape[0]
action_dim = env.action_space.shape[0]
device = "cuda" if torch.cuda.is_available() else "cpu"

agent = SAC(state_dim, action_dim, hidden_dim=128, device=device)
memory = MemoryBuffer(100000, device=device)
episode = 100
init_explor = 10000
batch_size = 64
__eval = True

t4steps = 0

for epi in range(episode):
    total_reward = 0
    total_steps = 0
    done = False
    state, _ = env.reset()

    while not done:
        if total_steps < init_explor:
            action = env.action_space.sample()
        else:
            action = agent.get_action(state)

        if len(memory) >= batch_size:
            critic_1_loss, critic_2_loss, policy_loss, ent_loss, alpha = agent.train(memory, batch_size)
            if (t4steps+1) % 500 == 0:
              print(f"Q1 loss: {critic_1_loss}, Q2 loss: {critic_2_loss}, Policy loss: {policy_loss}, Entropy loss: {ent_loss}, Alpha: {alpha}")

        next_state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated
        total_steps += 1
        t4steps += 1
        total_reward += reward

        memory.push(state, action, reward, next_state, truncated) # Append transition to memory

        state = next_state

    print("Episode: {}, episode steps: {}, reward: {}".format(epi+1, total_steps, round(total_reward, 2)))

    if (epi+1) % 10 == 0 and __eval is True:
        avg_reward = 0.
        episodes = 10
        for _  in range(episodes):
            state, _ = env.reset()
            episode_reward = 0
            done = False
            while not done:
                action = agent.get_action(state, eval=True)

                next_state, reward, terminated, truncated, _ = env.step(action)
                done = terminated or truncated
                episode_reward += reward
                state = next_state

            avg_reward += episode_reward
        avg_reward /= episodes


        print("----------------------------------------")
        print("Test Episodes: {}, Avg. Reward: {}".format(episodes, round(avg_reward, 2)))
        print("----------------------------------------")

        if avg_reward > 900:
            agent.save_checkpoint(ckpt_path=f"Q2_episode{epi+1}")


env.close()